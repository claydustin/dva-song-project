{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916d8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ed05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "from sklearn.feature_extraction import text## Stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e744fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfec33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a78f9",
   "metadata": {},
   "source": [
    "## 0. Pre-Processing - Remove Stop Words and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418a5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18438\\AppData\\Local\\Temp/ipykernel_18376/2218329103.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df_raw.drop(['Unnamed: 0','Unnamed: 5', 'Unnamed: 6'], 1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"../input/joined_data.csv\")\n",
    "df_raw.drop(['Unnamed: 0','Unnamed: 5', 'Unnamed: 6'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af64f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142643 \t Total Rows\n",
      "140323 \t Total Non-Duplicate Rows\n",
      "94117 \t Total Rows of Non-Duplicate Band/Songs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(df_raw)} \\t Total Rows\")\n",
    "print(f\"{len(df_raw.drop_duplicates())} \\t Total Non-Duplicate Rows\")\n",
    "print(f\"{len(df_raw.drop_duplicates(subset = ['cleaned_song', 'cleaned_band']))} \\t Total Rows of Non-Duplicate Band/Songs\")\n",
    "df = df_raw.drop_duplicates(subset=['cleaned_song', 'cleaned_band'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357ff74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = pd.read_csv(\"../input/stop words.csv\", encoding='unicode_escape')['stop word'].tolist()\n",
    "j = list(\",.!#[]/'\")\n",
    "\n",
    "stopwords = set(i).union(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a411f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = re.sub('[^a-z\\s]', '', str(x).lower()) # COMMENT: semi-equivalent to Baharks code: x.strip(\",.!#[];\\\"\\\"\\/\\'\\\\()!?$*+-><_~1234567890&:=^`\")\n",
    "    x = [w for w in x.split() if w not in stopwords]\n",
    "    \n",
    "    return ' '.join(x) or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377bcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18438\\AppData\\Local\\Temp/ipykernel_18376/2977327710.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_lyrics'] = df.apply(lambda x: preprocess(x.Lyrics), axis = 1)\n",
      "C:\\Users\\18438\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df['cleaned_lyrics'] = df.apply(lambda x: preprocess(x.Lyrics), axis = 1)\n",
    "df.dropna(subset=['cleaned_lyrics'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc12e88",
   "metadata": {},
   "source": [
    "### 0.1 Censor profanity, Edit \"TRUE\" and \"FALSE\", Stem, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the lyrics\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['stemmed_lyrics'] = df['cleaned_lyrics'].apply(lambda x: ' '.join([stemmer.stem(y) for y in nltk.word_tokenize(x)]))\n",
    "\n",
    "# Convert \"TRUE\" and \"FALSE\" to \"true\" and \"false\"\n",
    "df['cleaned_lyrics'] = df['cleaned_lyrics'].str.replace('TRUE','true')\n",
    "df['cleaned_lyrics'] = df['cleaned_lyrics'].str.replace('FALSE','false')\n",
    "df['stemmed_lyrics'] = df['stemmed_lyrics'].str.replace('TRUE','true')\n",
    "df['stemmed_lyrics'] = df['stemmed_lyrics'].str.replace('FALSE','false')\n",
    "\n",
    "# Edit profanity by replacing words with versions where vowels are replaced by asteriks\n",
    "profane_words = pd.read_table('../input/profane_words.txt', names=['words'])\n",
    "profane_words['edited_words'] = profane_words['words'].str.replace(r'\\B[aeiou]\\B','*')\n",
    "profane_words['profane_tuple'] = list(zip(profane_words['words'], profane_words['edited_words']))\n",
    "# Editing 'cleaned_lyrics'\n",
    "for tup in profane_words['profane_tuple']:\n",
    "    df['cleaned_lyrics'] = df['cleaned_lyrics'].str.replace(tup[0],tup[1])\n",
    "    df['stemmed_lyrics'] = df['stemmed_lyrics'].str.replace(tup[0],tup[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda82965",
   "metadata": {},
   "source": [
    "## 1. Remove Non-English Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(brown.words()) ## Cache\n",
    "\n",
    "def classify_english_songs(lyrics):\n",
    "    unique_words = set(lyrics.split())\n",
    "    sample_size = min(50, len(unique_words))\n",
    "    listed_lyrics = random.sample(set(lyrics.split()), sample_size)\n",
    "    english_lyrics = [w for w in listed_lyrics if w in english_words]\n",
    "    \n",
    "    return len(english_lyrics)/sample_size\n",
    "   \n",
    "df['english_percentage'] = df.apply(lambda x: classify_english_songs(x.cleaned_lyrics), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf42804",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['english_percentage'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['english_percentage'] < .6].to_csv(\"non_english_songs.csv\", index=False)\n",
    "print(f\"Removed {len(df[df['english_percentage'] < .6])} Non-English Songs\")\n",
    "df = df[df['english_percentage'] > .6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733bd19",
   "metadata": {},
   "source": [
    "## 2. Deduping song covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5b318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CASE 1: Two songs are the same (Count 5430)\n",
    "song_dupes = df.groupby(\"cleaned_song\").filter(lambda x: len(x) == 2) ##only keep songs that have one duplicate\n",
    "print(len(song_dupes)/2)\n",
    "song_dupes.sort_values(\"cleaned_song\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted = song_dupes.sort_values(\"year\").groupby('cleaned_song').shift(1)\n",
    "song_dupes_comp = song_dupes.join(shifted.rename(columns=lambda x: x+\"_lag\")).sort_values(\"cleaned_song\")\n",
    "song_dupes_comp = song_dupes_comp[['cleaned_song', 'Lyrics', 'Lyrics_lag', 'cleaned_band', 'cleaned_band_lag', 'year', 'year_lag', 'cleaned_lyrics', 'cleaned_lyrics_lag']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_dupes_comp.sort_values(\"cleaned_song\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35560350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(str_col: pd.Series,  max_features: int, ngram: str):\n",
    "    \"\"\"\n",
    "    input:  str_col - pd.Series -> a column of strings\n",
    "            ngram   - String    -> ngram option for bigrams\n",
    "    output: tuple     -> np.SparseMatrix with one-hot-encoding representation, tokenized and joined list of strings\n",
    "            ex. ([[1,0,1], [0,0,1]], [\"red,blue\",\"blue\"])\n",
    "\n",
    "    vectorizer.inverse_transform() returns a tokenized np.array of tokens.\n",
    "    ex.  returns -> ['document,first,is,the,this',\n",
    "             'document,is,second,the,this',\n",
    "             'and,is,one,the,third,this',\n",
    "             'document,first,is,the,this']\n",
    "    \"\"\"\n",
    "    if ngram == \"bigram\":\n",
    "        ngram_range = (2, 2)\n",
    "    else:\n",
    "        ngram_range = (1, 1)\n",
    "\n",
    "    list_col = str_col.tolist()\n",
    "    # COMMENT: could add token_pattern = r\"\\(.*?\\)|([^\\W_]+[^\\s-]*)\"\n",
    "    # COMMENT: sklearn.CountVectorizer -> Removes stop words and punctuation and represents words as list of counts\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\",\n",
    "                                 binary=True,\n",
    "                                 max_features=max_features,\n",
    "                                 ngram_range=ngram_range).fit(list_col)\n",
    "\n",
    "    x = vectorizer.transform(list_col)\n",
    "    tokenized = [\",\".join(token_array) for token_array in vectorizer.inverse_transform(x.toarray())]\n",
    "\n",
    "    return x, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lyric_similarity(lyric1_col: pd.Series, lyric2_col: pd.Series):\n",
    "    lyrics = pd.concat(\n",
    "        [lyric1_col.reset_index(drop=True), lyric2_col.reset_index(drop=True)],\n",
    "        ignore_index=True)\n",
    "    encoded_lyrics = one_hot_encoding(lyrics, ngram='unigram', max_features = None)[0].toarray()\n",
    "\n",
    "    lyrics_1 = encoded_lyrics[0:len(lyric1_col)]\n",
    "    lyrics_2 = encoded_lyrics[len(lyric1_col):]\n",
    "\n",
    "    cosine_scores = []\n",
    "    for i in range(lyrics_1.shape[0]):\n",
    "        score = cosine_similarity([lyrics_1[i]], [lyrics_2[i]])[0]\n",
    "        cosine_scores.append(score)\n",
    "\n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288822ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMMENT: True Covers\n",
    "song_dupes_comp['score'] = find_lyric_similarity(song_dupes_comp['cleaned_lyrics'], song_dupes_comp['cleaned_lyrics_lag'])\n",
    "song_dupes_comp[song_dupes_comp['score']>.95].head() ## COMMENT: 596 songs are actual covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = df.merge(song_dupes_comp[song_dupes_comp['score']>.95], how='left')\n",
    "print(f\"Found {len(unique[~unique['Lyrics_lag'].isnull()])} Song Cover duplicates\")\n",
    "unique = unique[unique['Lyrics_lag'].isnull()]\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081de26a",
   "metadata": {},
   "source": [
    "### 2.2 Pre-Processing for Market Basket Analysis (MBA) - \"only_popular_word\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6df589",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatlist = [list(set(sublist.split())) for sublist in unique['cleaned_lyrics'].tolist()]\n",
    "flatlist = [item for sublist in flatlist for item in sublist]\n",
    "## word count\n",
    "\n",
    "word_count = dict(Counter(flatlist))\n",
    "word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894966b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the purpose of graph\n",
    "## Top 20 words-First graph\n",
    "pop_words_graph = more_itertools.take(20, word_count.items())\n",
    "plt.figure(figsize=(10,7))\n",
    "x_val = [x[0] for x in pop_words_graph]\n",
    "y_val = [x[1] for x in pop_words_graph]\n",
    "plt.barh(x_val,y_val, color='#6495ED')\n",
    "plt.xlabel(\"Occurance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.rc('font', size=10) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Second graph\n",
    "pop_words_graph = more_itertools.take(10000, word_count.items())\n",
    "x = pd.Series([i[1] for i in pop_words_graph])\n",
    "plt.figure(figsize=(10,7))\n",
    "ax = sns.distplot(x, bins=50, kde=True, hist_kws=dict(color= \"#0000FF\", edgecolor=\"white\", linewidth=2),\\\n",
    "                 kde_kws=dict(color= \"orange\", linewidth=2))\n",
    "plt.xticks([600, 5000, 10000, 20000, 30000])\n",
    "plt.xlabel(\"Occurance\")\n",
    "\n",
    "\n",
    "less_1000 = len([i for i in x if i<600])\n",
    "less_1000/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## top 1200 words list is saved\n",
    "## 1200 seems the good threshhold\n",
    "\n",
    "pop_words_selected = more_itertools.take(1200, word_count.items())\n",
    "pop_words_selected = [x[0] for x in pop_words_selected]\n",
    "\n",
    "with open('../output/word_list.csv', mode='w') as word_file:\n",
    "    word_file = csv.writer(word_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    word_file.writerow(pop_words_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## select lyrics that have the top words\n",
    "def popular_words(row, popular):\n",
    "\n",
    "        return [w for w in row.split() if w in popular]\n",
    "    \n",
    "unique['only_popular_word'] = unique.apply(lambda x: popular_words(x['cleaned_lyrics'], pop_words_selected), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db95b0a",
   "metadata": {},
   "source": [
    "## 3. Picking Anchor Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT: Models changed on different time-periods\n",
    "period_1 = unique[unique['year'] < 1983]\n",
    "period_2 = unique[(unique['year'] >= 1983) & (unique['year'] <=1991)]\n",
    "period_3 = unique[(unique['year'] > 1991)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04023cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(flatlist, n):\n",
    "    ## word count\n",
    "    word_count = Counter(flatlist)\n",
    "    word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    top_100 = list(word_count.keys())[0:(n-1)]\n",
    "    \n",
    "    return top_100\n",
    "    \n",
    "flat_p1 = list(itertools.chain(*[lyrics.split() for lyrics in period_1['cleaned_lyrics'].tolist()]))\n",
    "flat_p2 = list(itertools.chain(*[lyrics.split() for lyrics in period_2['cleaned_lyrics'].tolist()]))\n",
    "flat_p3 = list(itertools.chain(*[lyrics.split() for lyrics in period_3['cleaned_lyrics'].tolist()]))\n",
    "\n",
    "top_100_p1 = get_top_n(flat_p1, 250)\n",
    "top_100_p2 = get_top_n(flat_p2, 250)\n",
    "top_100_p3 = get_top_n(flat_p3, 250)\n",
    "\n",
    "top_i = 0\n",
    "j = 0\n",
    "top_100_words = []\n",
    "while top_i < 100:\n",
    "    if (top_100_p1[j] in top_100_p2) and (top_100_p1[j] in top_100_p2):\n",
    "        top_100_words.append(top_100_p1[j])\n",
    "        top_i+=1\n",
    "    j += 1\n",
    "\n",
    "X = set(top_100_p1).intersection(top_100_p2).intersection(top_100_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_words = ['love', 'night', 'man', 'day', 'heart', 'life', 'world', 'woman', 'dance', 'dream', 'lonely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b19f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_words = top_100_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2be06",
   "metadata": {},
   "source": [
    "## 4. Co-Occurence (counted weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4558182",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flat list for periods\n",
    "flatList_p1 = list(period_1[\"only_popular_word\"])\n",
    "flatList_p2 = list(period_2[\"only_popular_word\"])\n",
    "flatList_p3 = list(period_3[\"only_popular_word\"])\n",
    "\n",
    "period_map = {1: \"Before 1983\", 2: \"Between 1983 and 1991\", 3: \"After 1991\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70faf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cout co occurances in period 1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "def cooccurrence(items):\n",
    "    \"\"\"\n",
    "    Updates a dictionary of pair counts for\n",
    "    all pairs of items in a given itemset.\n",
    "    \"\"\"\n",
    "    pair_counts = defaultdict(int)\n",
    "    for item in items:\n",
    "        for (a, b) in itertools.combinations(item, 2):\n",
    "            if a != b:\n",
    "                sorted_comb = sorted((a,b))\n",
    "                tuple_comb = tuple(sorted_comb)\n",
    "\n",
    "                pair_counts[tuple_comb] += 1\n",
    "\n",
    "    return pair_counts\n",
    "\n",
    "for i, period in enumerate([flatList_p1, flatList_p2, flatList_p3]):\n",
    "    cooccurrence_dict = cooccurrence(period)\n",
    "    first_word = [w1 for w1,w2 in cooccurrence_dict.keys()]\n",
    "    second_word = [w2 for w1,w2 in cooccurrence_dict.keys()]\n",
    "    df = pd.DataFrame({'first_word':first_word\n",
    "                  , 'second_word': second_word\n",
    "                  , 'weight': list(cooccurrence_dict.values())})\n",
    "    \n",
    "    df['scaled_weight'] = scaler.fit_transform(df[['weight']])\n",
    "    df['method'] = 'Co-Occurrence by Count'\n",
    "    df['period'] = period_map[i+1]\n",
    "    \n",
    "    df.to_csv(f'../output/node_edge_p{i+1}.csv', index=False)\n",
    "    output_df = pd.concat([output_df, df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d54f73",
   "metadata": {},
   "source": [
    "## 5. Co-Occurrence (MBA support weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "ccebf0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, period in enumerate([flatList_p1, flatList_p2, flatList_p3]):\n",
    "    algo = apriori(period, min_support=0.001, min_confidence=0.001, min_lift=2, max_length=2)\n",
    "    results = list(algo)\n",
    "\n",
    "    ## pd.DataFrame(results) and save\n",
    "    association_rule = pd.DataFrame(results).sort_values(by=['support'], ascending=False)\n",
    "    first_word = []\n",
    "    second_word = []\n",
    "    for _, row in association_rule.iterrows():\n",
    "        w1, w2 = tuple(row['items'])\n",
    "        first_word.append(w1)\n",
    "        second_word.append(w2)\n",
    "    df = pd.DataFrame({'first_word': first_word, 'second_word': second_word, 'weight': association_rule.support})\n",
    "    \n",
    "    df['scaled_weight'] = scaler.fit_transform(df[['weight']])\n",
    "    df['method'] = 'Co-Occurence MBA'\n",
    "    df['period'] = period_map[i+1]\n",
    "    \n",
    "    df.sort_values(by='weight', ascending=False).to_csv(f'../output/association_rule_P{i+1}.csv', index=False)   \n",
    "    output_df = pd.concat([output_df, df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab181a3",
   "metadata": {},
   "source": [
    "## 6. kNN - Cosine Similarity of Word2Vec Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "28dd209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LINK: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "for i, period in enumerate([period_1, period_2, period_3]):\n",
    "    lyric_list = [song_lyrics.split() for song_lyrics in period['cleaned_lyrics'].tolist()]\n",
    "    model = Word2Vec(sentences=lyric_list, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "    vocab = set(anchor_words)\n",
    "    for word in anchor_words:\n",
    "        word2vec_top = model.wv.most_similar(word, topn=10)\n",
    "        vocab = vocab.union([w for w,p in word2vec_top])\n",
    "\n",
    "    rows = []\n",
    "    for (a, b) in itertools.combinations(vocab, 2):\n",
    "        if a != b:\n",
    "            weight = model.wv.similarity(a,b)\n",
    "            rows.append([a,b,weight])\n",
    "\n",
    "    word2vec_df = pd.DataFrame(rows, columns=['first_word', 'second_word', 'weight'])\n",
    "    \n",
    "\n",
    "    ##word2vec_df = word2vec_df[word2vec_df['weight']>.75]\n",
    "    word2vec_df['scaled_weight'] = scaler.fit_transform(word2vec_df[['weight']])\n",
    "    \n",
    "    word2vec_df.to_csv(f\"../output/word2vec_p{i+1}.csv\", index=False)\n",
    "    word2vec_df['method'] = 'Word2Vec Cosine Similarity'\n",
    "    word2vec_df['period'] = period_map[i+1]\n",
    "    \n",
    "    \n",
    "    output_df = pd.concat([output_df, word2vec_df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6a7f9",
   "metadata": {},
   "source": [
    "# 7. Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "6a7d5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_nodes(df, limit):\n",
    "    counter = 0\n",
    "    vocab = []\n",
    "\n",
    "    for i, row in df.sort_values('scaled_weight', ascending=False).iterrows():\n",
    "        first_word = row['first_word']\n",
    "        second_word = row['second_word']\n",
    "        \n",
    "        if first_word not in vocab:\n",
    "            vocab.append(first_word)\n",
    "            counter+=1\n",
    "        \n",
    "        if second_word not in vocab:\n",
    "            vocab.append(second_word)\n",
    "            counter+=1\n",
    "        \n",
    "        if counter >= limit:\n",
    "            return vocab\n",
    "    \n",
    "def get_top_edges(node_list, df):\n",
    "    return df[df['first_word'].isin(node_list) & df['second_word'].isin(node_list)]\n",
    "\n",
    "\n",
    "def get_edges_for_word(word, df):\n",
    "    top_word_edges = pd.DataFrame()\n",
    "    for period in df.period.unique():\n",
    "        for method in df.method.unique():\n",
    "            sub_df = df[(df.method==method) & (df.period==period)]\n",
    "            sub_df = sub_df[(sub_df.first_word==word) | (sub_df.second_word==word)]\n",
    "            sub_df = sub_df.sort_values('scaled_weight', ascending=False)[0:5]\n",
    "            top_word_edges = pd.concat([top_word_edges, sub_df], ignore_index =True)\n",
    "            \n",
    "    return top_word_edges\n",
    "\n",
    "\n",
    "def set_weight_color(threshold_df, color_probs = [.80, .90, 1]):\n",
    "    size = len(threshold_df)\n",
    "    col1 = round(size*color_probs[0])\n",
    "    col2 = round(size*color_probs[1])\n",
    "    \n",
    "    threshold_df.reset_index(inplace=True)\n",
    "    threshold_df['color'] = 'color1'\n",
    "    \n",
    "    threshold_df.loc[threshold_df.index > col1, 'color'] = 'color2'\n",
    "    threshold_df.loc[threshold_df.index > col2, 'color'] = 'color3'\n",
    "    \n",
    "    return threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "64a844db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cooccurrence Edge Threshold 0.045845761\n",
    "threshold_df = pd.DataFrame()\n",
    "for method in output_df['method'].unique():\n",
    "    for period in output_df['period'].unique():\n",
    "        sub_df = output_df[(output_df.method==method) & (output_df.period==period)]\n",
    "        top_nodes = get_top_nodes(sub_df, 100)\n",
    "        top_edges = get_top_edges(top_nodes, sub_df).sort_values('scaled_weight', ascending=False)[0:499]\n",
    "\n",
    "        threshold_ = set_weight_color(top_edges)\n",
    "        threshold_df = pd.concat([threshold_df, threshold_], ignore_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "4f72f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_df.to_csv(\"../output/threshold_df.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
